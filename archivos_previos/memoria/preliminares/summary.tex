% !TeX root = ../libro.tex
% !TeX encoding = utf8
%
%*******************************************************
% Summary
%*******************************************************

\selectlanguage{english}
\chapter{Summary}
This project is about a very important result on differential topology. The main goal is to transmit how complex is the proof for a student, but been more comprehensible via Allen Hatcher's article \cite{arXiv:1312.3518}. Also, we will translate this problem to the computer scope, showing how difficult is to represent a surface correctly.\\
\\A typical problem of differential topology is known if it is possible to find a unique smooth structure for every topological surface (except diffeomorphisms). J. R. Munkres proved that result in his thesis in $1955$ but using some sophisticated analytical techniques. Allen Hatcher used ``The Kirby Torus Trick'' (a technique proposed by Kirby and Shiermann in $1960$) to proof the theorem for $2$-dimensional manifolds.\\
\\The initial main goals are to implement and design an efficient program to visualize homotopies, and proof the theorem as a corollary of the $2$ results cited on Allen Hatcher's article \cite{arXiv:1312.3518}. I think I have reached both goals and more because I found some interesting functionalities, like using partials of a defined function on the parametrization, and been able to visualize some characteristics of Morse functions (actually, only for ``Height'' function). However, it would have been interesting to have deepened more in Morse theory, but it would have required an exclusive project.\\

\section*{Mathematical part}
I will introduce some concepts and results about differential topology, principally from the subjects Topology II and Smooth Manifolds, and obviously Morse theory. After that, I enunciate the important results of this part, the ``facts'' (six facts). Those results are supposed by the author (Allen Hatcher) for the proof of the ``Handle Smoothing Theorem'' and theorems A and B (the $2$ main results whose corollary is the objective theorem). He proofs them at the end of his article.\\
\\Those facts need a large knowledge about Morse theory. To understand them, I used an article that concentrates some important results \cite{MorseTh1}. Also, it gives summarized proofs for almost all of them.\\
\\Then, I enunciate and extend the proof of the ``Handle Smoothing Theorem'', whose objective is to smooth one embedding incrementally, via finding isotopies (first on the origin, then on the segment $[0,1]$ and finally on the disk). It will help us to build a smooth structure starting from a base chart (topological chart, but being the only one it trivially defines a smooth structure) because we need a set of charts whose transition maps are smooth.\\
\\On the proof of that theorem, the author introduces the ``Torus trick'' (The Kirby Torus Trick), bringing up the structure of the surface to the Torus using an immersion to smooth the induced structure and bringing it back to the original surface (plus other steps). This together with Morse theory (specially, the ``facts'') are the key to build the entire proof of the theorems A and B because the rest needs only a correct connection.\\
\\Thanks to the previous theorem and the facts, I will enunciate and extend the proofs of theorems A and B (remember that in our case a manifold is a $2$-manifold):
\begin{itemize}
	\item \textbf{Theorem A}: Every manifold has a smooth structure.
	\item \textbf{Theorem B}: Every homeomorphism between smooth manifolds can be isotoped to a diffeomorphism.
\end{itemize}
For theorem A, using a triangulation of the open set of the transition map's preimage (supposing that is not smooth), we will be able to use the ``Handle Smoothing Theorem'' first on the vertex, then on the edges and finally on the interiors of the triangles. Therefore, we will have an isotopy of the main chart (whose have been affected by non-smooth transition map). Equivalently, we will have a chart with a smooth transition map with the previous charts. Then, by induction, we will define a smooth structure of the manifold, starting with a topological one.\\
\\Theorem B's proof is very similar to the proof of theorem A. We only need to translate chart smoothing for smoothing the homeomorphism, using the fact that exists a smooth triangulation on every smooth manifold.\\
\\Eventually, the central theorem of the project is a trivial corollary of theorems A and B.

\section*{Computational part}
Before explaining anything about this part, like the first part I will enunciate some useful concepts about $3$D visualization, especially OpenGL (Khronos Wiki \cite{KhronosWiki} was the main source in this topic).\\
\\After that, I begin with the development of tessellation study, in other words, how the program will subdivide the triangles to approximate better the surface. The first thing to do is select the correct definition of ``good surface approach'', under some threshold $\epsilon>0$.\\
\\Thanks to computer vision we know that the human vision perceives a surface through its geometry, edges, and textures. Because I use plain textures in this case and the charts defines a unique surface (only $1$ geometry element), we can reduce vision perception elements to edges and texture (how the light reacts to the surface). Abbreviating, the program will tessellate depending on the curvature (Gauss curvature, K, is the first characteristic of a surface that directly influences the light) and it will tessellate more on edges and areas with more light (dark areas will not be tessellated).\\
\\Also, I have implemented some typical techniques to improve the efficiency of the program, like adding tessellation distance (max distance to tessellate a triangle) or avoiding tessellation out of view frustum (out of the user's view). In some special cases we can also use an additional improvement, that prevent tessellate on hidden triangles. E. g. if we have a sphere, some triangles are visible on the front and others are not, on the back, then the program will not tessellate them.\\
\\Previously, I have studied more definitions, but the results were bad, because it was not able to detect critical areas like peaks (volume-based definition) or tessellates when it was not necessary (area-based definition).\\
\\In practice, before starting with tessellation study, it was needed to implement a program that pre-processes the charts (that defines the surface), giving as output a translation to GLSL language (used on the shaders) with additional functions, like the normal function of the charts and Gaussian curvature. For that, the program must recognize the expression trees of the functions and be able to derivate them (partial derivative respect to $x_i$, where $x_i$ is a function's variable). That GLSL code will be included in the shaders that will need it (tessellation shaders).\\
\\For the implementation of the ``processor'' I used a base code, provided in the subject Language Processors. It gave me an initial implementation that, after adapting it, can detect lexical, syntactical, and semantical errors.\\
\\In case of the visualizer, I used some example code that implements a basic shader for a simple $3$D scene (one object with one light, using Phong lighting). For its interface I used a free software library called Dear ImGui \cite{DearImGui}, with some addons for showing the light vector \cite{LVector} and a file selection dialog \cite{Dialog}, to select a parametrization.\\
\\The measures used to know if the program with tessellation works better than not using tessellation, have been the frames per second (how many frames have been calculated on one second) and the number of generated primitives. I want a program that visualizes the similar surface using less triangles than uniform approximation (with another initial mesh size), and generating more fps. This would mean that the program manages all the resources efficiently.\\
\\Because there are a lot of shading code that is executed many times (once per primitive or per vertex), it is required the best possible optimization. On Khronos Wiki \cite{KhronosWiki} there is some information about that. The more important thing is to avoid repeated calculations, like the inverse of a constant matrix (constant during rendering one frame). Apart of that, we can group some scalar calculations in a vector operation, reducing the number of operations (the number of ticks to resolve a vector operation is similar than a scalar operation, due to the GPU characteristics).\\

\section*{Conclusions}
After doing all the project, I can conclude that Allen Hatcher's article \cite{arXiv:1312.3518} provides a lot of results, which are so important in differential topology, supported by a large and interesting theory (Morse theory). It would be interesting going deeper into that theory, but it is very complex and will require a lot of time (it has content as for an own subject).\\
\\Respecting to the program, I realized how difficult is to correctly define a good approximation of a surface. Also, I have noticed that this program would be able to have more functionalities, like adding the possibility of using partials in the parametrization (implemented) and visualize Morse functions, and some characteristics (like critical points and, possibly, them indices).\\
\\There is a topic that I have not been able to study, because of the time. It is ``Transform feedback'', a functionality of OpenGL that allows render again the output mesh. It would add a lot of possibilities, including recursive tessellation (the originally thought algorithm). Logically, it will have a high negative impact on efficiency, but it should be studied carefully.\\
\\Also, we could modify the program to give as output a mesh that represents a surface with a certain number of primitives as goal. It would improve the performance of the program if we are treating with static objects (only surfaces, not homotopies), avoiding all the calculations for each vertex (calculate image through parameterization and the normal through the normal function of the parameterization).\\
\\It would be a good idea to implement the program with WebGL, to make it more portable and flexible.\\
\\These are the key words of the project:
\begin{itemize}
	\item Surface.
	\item Deepen.
	\item Comprehension.
	\item Visualization.
	\item Efficiency.
\end{itemize}

% Al finalizar el resumen en inglés, volvemos a seleccionar el idioma español para el documento
\selectlanguage{spanish} 
\endinput
